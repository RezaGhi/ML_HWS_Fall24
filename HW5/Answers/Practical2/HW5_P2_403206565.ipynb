{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"73b707972feb43fbb12cf69cb5355c4d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2918e1e9113a4505aaef1a08e43aa831","IPY_MODEL_a3c98f8ab89b470da132a0ecd330df06","IPY_MODEL_233d606f46d240c782b3a8b788c773c2"],"layout":"IPY_MODEL_5a93f52b57e842089e75b6dbb5b977a5"}},"2918e1e9113a4505aaef1a08e43aa831":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fc5fb28327f41d598f465c3f7d0c056","placeholder":"​","style":"IPY_MODEL_c0b0712d9d834e74a05e7ffc64b4ee78","value":"tokenizer_config.json: 100%"}},"a3c98f8ab89b470da132a0ecd330df06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3242d33f28b446a1bcd8774cb49b8d3a","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26b46e91437844b9a287b159f6165b6b","value":48}},"233d606f46d240c782b3a8b788c773c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82eb9cada13b40b2859eb3f8f4c342ec","placeholder":"​","style":"IPY_MODEL_6f416dcbfcb141bf8177df2f34bc3158","value":" 48.0/48.0 [00:00&lt;00:00, 2.85kB/s]"}},"5a93f52b57e842089e75b6dbb5b977a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fc5fb28327f41d598f465c3f7d0c056":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0b0712d9d834e74a05e7ffc64b4ee78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3242d33f28b446a1bcd8774cb49b8d3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26b46e91437844b9a287b159f6165b6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82eb9cada13b40b2859eb3f8f4c342ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f416dcbfcb141bf8177df2f34bc3158":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"facaf403897e4599b31a64967d284ae7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8b25a3fecf994184a38322ed4d06e970","IPY_MODEL_dc1e88e0a4764c13b0789f5ac65476a1","IPY_MODEL_ee1557e128fc40b8a4a1d349d027296c"],"layout":"IPY_MODEL_3e3168e32ff545a5b9910c21f93acedb"}},"8b25a3fecf994184a38322ed4d06e970":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_932da40830274fb99bf2e4e0c603dec2","placeholder":"​","style":"IPY_MODEL_8a35fdf7e9d74791b806d8d37472a560","value":"vocab.txt: 100%"}},"dc1e88e0a4764c13b0789f5ac65476a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ca64dc57a034ddba0ee30717669e442","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_845b8b99a9624a9e807d830cf2da178e","value":231508}},"ee1557e128fc40b8a4a1d349d027296c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6ffd3f63f054a8b8dfd6ab8aa685c52","placeholder":"​","style":"IPY_MODEL_7f4201f58cef491bae7bfe5e986a9bf7","value":" 232k/232k [00:00&lt;00:00, 3.18MB/s]"}},"3e3168e32ff545a5b9910c21f93acedb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"932da40830274fb99bf2e4e0c603dec2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a35fdf7e9d74791b806d8d37472a560":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ca64dc57a034ddba0ee30717669e442":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"845b8b99a9624a9e807d830cf2da178e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6ffd3f63f054a8b8dfd6ab8aa685c52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f4201f58cef491bae7bfe5e986a9bf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f02fd62a23c84d64b29276ed416a4242":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2744749ee8e5461eac45ede5ace2b0ba","IPY_MODEL_fde93033c8fb49418560da16eda6f3f9","IPY_MODEL_197ecd94028340fe9563b204f2794b97"],"layout":"IPY_MODEL_1fa068ffd78d4d3eb33da754a3f629af"}},"2744749ee8e5461eac45ede5ace2b0ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_077edd9f32b94989a2de33cef9ada369","placeholder":"​","style":"IPY_MODEL_4e0e6b6c217843259e0f5794afa0a409","value":"tokenizer.json: 100%"}},"fde93033c8fb49418560da16eda6f3f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_595d8bc667c741af8bb530fbd831d5d9","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69cc16a8ae4744c38133169d70decf09","value":466062}},"197ecd94028340fe9563b204f2794b97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e07aa8912428436d91dca6fb0f7e1043","placeholder":"​","style":"IPY_MODEL_7c15e6d375b0419f9295a8643f2eb72c","value":" 466k/466k [00:00&lt;00:00, 3.51MB/s]"}},"1fa068ffd78d4d3eb33da754a3f629af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"077edd9f32b94989a2de33cef9ada369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e0e6b6c217843259e0f5794afa0a409":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"595d8bc667c741af8bb530fbd831d5d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69cc16a8ae4744c38133169d70decf09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e07aa8912428436d91dca6fb0f7e1043":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c15e6d375b0419f9295a8643f2eb72c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2be5cd6b2a1b4b269904946662be8aab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a279cce661242a6b163a509f54985c2","IPY_MODEL_4b3c66662c5a42ffa06878ac94a3051f","IPY_MODEL_02d782033a874e62a8f9b3e73d1c0473"],"layout":"IPY_MODEL_a88cad9a904a40cf82ac3440f3cfc629"}},"7a279cce661242a6b163a509f54985c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fd8d4bbe7f2469297f2b226a8876e26","placeholder":"​","style":"IPY_MODEL_8788c3b5bee748b58536b89cca1cea03","value":"config.json: 100%"}},"4b3c66662c5a42ffa06878ac94a3051f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b70e1d4944846f5b09188e103c26174","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1389a52ea82447d4a9e9947cb6872a0f","value":570}},"02d782033a874e62a8f9b3e73d1c0473":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c2dfd57a528415086d53a9fa73fab14","placeholder":"​","style":"IPY_MODEL_d1af3cb26a004c1e8784ee1357ed4783","value":" 570/570 [00:00&lt;00:00, 40.8kB/s]"}},"a88cad9a904a40cf82ac3440f3cfc629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fd8d4bbe7f2469297f2b226a8876e26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8788c3b5bee748b58536b89cca1cea03":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b70e1d4944846f5b09188e103c26174":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1389a52ea82447d4a9e9947cb6872a0f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c2dfd57a528415086d53a9fa73fab14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1af3cb26a004c1e8784ee1357ed4783":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2938ae2372645bb8d802f5584a58264":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_772fd0c1f3224c3a8dcc7c98e165971a","IPY_MODEL_c10c0b5b2ae24d709a96d01fa600b749","IPY_MODEL_81a759535ea144c8ace92715cc7fd7c1"],"layout":"IPY_MODEL_4a45fea2fa5441e0abc15137e5b1421f"}},"772fd0c1f3224c3a8dcc7c98e165971a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea2fb54ef8934b88bde0a94364613aa6","placeholder":"​","style":"IPY_MODEL_fbcf857dc50c45f2a41226287e67c18e","value":"config.json: 100%"}},"c10c0b5b2ae24d709a96d01fa600b749":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32a14f57bddc44efa7a909763f60f4ca","max":285,"min":0,"orientation":"horizontal","style":"IPY_MODEL_608f0f0f4133479ab29673fe939bafec","value":285}},"81a759535ea144c8ace92715cc7fd7c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b35e44afa1954874b632cca1d1efba61","placeholder":"​","style":"IPY_MODEL_1e6a0501fc65467ba214a09daf70a81f","value":" 285/285 [00:00&lt;00:00, 23.7kB/s]"}},"4a45fea2fa5441e0abc15137e5b1421f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea2fb54ef8934b88bde0a94364613aa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbcf857dc50c45f2a41226287e67c18e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32a14f57bddc44efa7a909763f60f4ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"608f0f0f4133479ab29673fe939bafec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b35e44afa1954874b632cca1d1efba61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e6a0501fc65467ba214a09daf70a81f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e4f9dd7dc4f4b31956c8d884c55d48b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43e8eb0792d84975ae609ee5cffe2d84","IPY_MODEL_69f34c518b6f49e6904d037b3ec41a4f","IPY_MODEL_fd8906336229485fba037de182ff9265"],"layout":"IPY_MODEL_bb2bea62ebdf4be69b8a61f78a0862f3"}},"43e8eb0792d84975ae609ee5cffe2d84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8824952366bf4301b561cc3a352790d0","placeholder":"​","style":"IPY_MODEL_f53816b45d1c4b3a8e8f645a11c7ca98","value":"pytorch_model.bin: 100%"}},"69f34c518b6f49e6904d037b3ec41a4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff2cd36fe38b44429ee08e08c4ad9cde","max":17756393,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e18c04dd8f3405e94f44f65332ff841","value":17756393}},"fd8906336229485fba037de182ff9265":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce12ed992d0942acb809f917e03c4c94","placeholder":"​","style":"IPY_MODEL_1c33426f0ec0411599ce1828ccb1823d","value":" 17.8M/17.8M [00:00&lt;00:00, 41.6MB/s]"}},"bb2bea62ebdf4be69b8a61f78a0862f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8824952366bf4301b561cc3a352790d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f53816b45d1c4b3a8e8f645a11c7ca98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff2cd36fe38b44429ee08e08c4ad9cde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e18c04dd8f3405e94f44f65332ff841":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce12ed992d0942acb809f917e03c4c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c33426f0ec0411599ce1828ccb1823d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<font>\n<div dir=ltr align=center>\n<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n<font color=0F5298 size=7>\n    Machine learning <br>\n<font color=2565AE size=5>\n    Computer Engineering Department <br>\n    Fall 2024<br>\n<font color=3C99D size=5>\n    Practical Assignment 5 - NLP - Transformer & Bert <br>\n</div>\n<div dir=ltr align=center>\n<font color=0CBCDF size=4>\n   &#x1F349; Masoud Tahmasbi  &#x1F349;  &#x1F353; Arash Ziyaei &#x1F353;\n<br>\n<font color=0CBCDF size=4>\n   &#x1F335; Amirhossein Akbari  &#x1F335;\n</div>\n\n____","metadata":{"id":"VivaFsd3Q6cj"}},{"cell_type":"markdown","source":"<font color=9999FF size=4>\n&#x1F388; Full Name : Reza ghobani Paji\n<br>\n<font color=9999FF size=4>\n&#x1F388; Student Number : 403206565","metadata":{"id":"k15QziPnmC6d"}},{"cell_type":"markdown","source":"<font color=0080FF size=3>\nThis notebook covers two key topics. First, we implement a transformer model from scratch and apply it to a specific task. Second, we fine-tune the BERT model using LoRA for efficient adaptation to a downstream task.\n</font>\n<br>\n\n**Note:**\n<br>\n<font color=66B2FF size=2>In this notebook, you are free to use any function or model from PyTorch to assist with the implementation. However, TensorFlow is not permitted for this exercise. This ensures consistency and alignment with the tools being focused on.</font>\n<br>\n<font color=red size=3>**Run All Cells Before Submission**</font>: <font color=FF99CC size=2>Before saving and submitting your notebook, please ensure you run all cells from start to finish. This practice guarantees that your notebook is self-consistent and can be evaluated correctly by others.</font>","metadata":{"id":"IOfpEN2xmbN8"}},{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install datasets\n!pip install torchmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:22.774480Z","iopub.execute_input":"2025-01-19T14:22:22.774733Z","iopub.status.idle":"2025-01-19T14:22:33.655062Z","shell.execute_reply.started":"2025-01-19T14:22:22.774704Z","shell.execute_reply":"2025-01-19T14:22:33.653987Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Section 1: Transformer\n\nThe transformer architecture consists of two main components: an encoder and a decoder. Each of these components is made up of multiple layers that include self-attention mechanisms and feedforward neural networks. The self-attention mechanism is central to the transformer, as it enables the model to assess the importance of different words in a sentence by considering their relationships with one another.\n\n\nIn this assignment, you should design a transformer model from scratch. You are required to implement the Encoder and Decoder components of a Transformer model.","metadata":{"id":"SpvvM0995ieR"}},{"cell_type":"code","source":"# Importing libraries\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nfrom datasets import load_dataset\nimport torchmetrics\n\n# Math\nimport math\n\n# HuggingFace libraries\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\n# Pathlib\nfrom pathlib import Path\n\n# typing\nfrom typing import Any\n\n# Library for progress bars in loops\nfrom tqdm import tqdm\n\n# Importing library of warnings\nimport warnings","metadata":{"id":"dzIob6-Gq7Lw","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:33.659050Z","iopub.execute_input":"2025-01-19T14:22:33.659338Z","iopub.status.idle":"2025-01-19T14:22:54.680547Z","shell.execute_reply.started":"2025-01-19T14:22:33.659297Z","shell.execute_reply":"2025-01-19T14:22:54.679869Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Part 1: Input Embeddings\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we observe the Transformer architecture image above, we can see that the Embeddings represent the first step of both blocks.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>InputEmbedding</code> class below is responsible for converting the input text into numerical vectors of <code>d_model</code> dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{model}}$.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the image below, we can see how the embeddings are created. First, we have a sentence that gets split into tokens—we will explore what tokens are later on—. Then, the token IDs—identification numbers—are transformed into the embeddings, which are high-dimensional vectors.</p>","metadata":{"id":"-71SfIAJprox"}},{"cell_type":"code","source":"class InputEmbeddings(nn.Module):\n\n    def __init__(self, d_model: int, vocab_size: int) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embedding(x) * math.sqrt(self.d_model)","metadata":{"id":"J-pyrJlu4Nl7","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.681323Z","iopub.execute_input":"2025-01-19T14:22:54.682035Z","iopub.status.idle":"2025-01-19T14:22:54.686748Z","shell.execute_reply.started":"2025-01-19T14:22:54.682005Z","shell.execute_reply":"2025-01-19T14:22:54.685766Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Part 2: positional encoding\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder blocks so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>PositionalEncoding</code> class below, we will create a matrix of positional encodings <code>pe</code> with dimensions <code>(seq_len, d_model)</code>. We will start by filling it with $0$s.We will then apply the sine function to even indices of the positional encoding matrix while the cosine function is applied to the odd ones.</p>\n\n<p style=\"\n    margin-bottom: 5;\n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000;\n  \">\n    \\begin{equation}\n    \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n    \\end{equation}\n</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We apply the sine and cosine functions because it allows the model to determine the position of a word based on the position of other words in the sequence, since for any fixed offset $k$, $PE_{pos + k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of sine and cosine functions, where a shift in the input results in a predictable change in the output.</p>","metadata":{"id":"RWBlo2XorJGW"}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        pe = torch.zeros(seq_len, d_model)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)","metadata":{"id":"4ZG5DhVcrVVm","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.689210Z","iopub.execute_input":"2025-01-19T14:22:54.689473Z","iopub.status.idle":"2025-01-19T14:22:54.704455Z","shell.execute_reply.started":"2025-01-19T14:22:54.689451Z","shell.execute_reply":"2025-01-19T14:22:54.703647Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Part 3: layer normalization\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the encoder and decoder blocks, we see several normalization layers called <b><i>Add &amp; Norm</i></b>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>LayerNormalization</code> class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero. This process results in a normalized output with a mean 0 and a standard deviation 1.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will then scale the normalized output by a learnable parameter <code>alpha</code> and add a learnable parameter called <code>bias</code>. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent.</p>","metadata":{"id":"T92iydQErh-P"}},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n\n    def __init__(self, features: int, eps:float=10**-6) -> None:\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(features))\n        self.bias = nn.Parameter(torch.zeros(features))\n\n    def forward(self, x):\n        mean = x.mean(dim = -1, keepdim = True)\n        std = x.std(dim = -1, keepdim = True)\n        return self.alpha * (x - mean) / (std + self.eps) + self.bias","metadata":{"id":"kVGQRsmKrwZu","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.705995Z","iopub.execute_input":"2025-01-19T14:22:54.706301Z","iopub.status.idle":"2025-01-19T14:22:54.719150Z","shell.execute_reply.started":"2025-01-19T14:22:54.706273Z","shell.execute_reply":"2025-01-19T14:22:54.718318Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Part 4: Feed Forward Network\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the fully connected feed-forward network, we apply two linear transformations with a ReLU activation in between. We can mathematically represent this operation as:</p>\n\n<p style=\"\n    margin-bottom: 5;\n    font-size: 22px;\n    font-weight: 300;\n    font-family: 'Helvetica Neue', sans-serif;\n    color: #000000;\n  \">\n    \\begin{equation}\n    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n    \\end{equation}\n</p>\n\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">$W_1$ and $W_2$ are the weights, while $b_1$ and $b_2$ are the biases of the two linear transformations.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>FeedForwardBlock</code> below, we will define the two linear transformations—<code>self.linear_1</code> and <code>self.linear_2</code>—and the inner-layer <code>d_ff</code>. The input data will first pass through the <code>self.linear_1</code> transformation, which increases its dimensionality from <code>d_model</code> to <code>d_ff</code>. The output of this operation passes through the ReLU activation function, which introduces non-linearity so the network can learn more complex patterns, and the <code>self.dropout</code> layer is applied to mitigate overfitting. The final operation is the <code>self.linear_2</code> transformation to the dropout-modified tensor, which transforms it back to the original <code>d_model</code> dimension.</p>","metadata":{"id":"U-IbSGQMr1Ye"}},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))","metadata":{"id":"N3H8kyccsEUW","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.720126Z","iopub.execute_input":"2025-01-19T14:22:54.720500Z","iopub.status.idle":"2025-01-19T14:22:54.737270Z","shell.execute_reply.started":"2025-01-19T14:22:54.720446Z","shell.execute_reply":"2025-01-19T14:22:54.736499Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Part 5: Multi Head Attention\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention is the most crucial component of the Transformer. It is responsible for helping the model to understand complex relationships and patterns in the data.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The image below displays how the Multi-Head Attention works. It doesn't include <code>batch</code> dimension because it only illustrates the process for one single sentence.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention block receives the input data split into queries, keys, and values organized into matrices $Q$, $K$, and $V$. Each matrix contains different facets of the input, and they have the same dimensions as the input.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We then linearly transform each matrix by their respective weight matrices $W^Q$, $W^K$, and $W^V$. These transformations will result in new matrices $Q'$, $K'$, and $V'$, which will be split into smaller matrices corresponding to different heads $h$, allowing the model to attend to information from different representation subspaces in parallel. This split creates multiple sets of queries, keys, and values for each head.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Finally, we concatenate every head into an $H$ matrix, which is then transformed by another weight matrix $W^o$ to produce the multi-head attention output, a matrix $MH-A$ that retains the input dimensionality.</p>","metadata":{"id":"YEa1kF6csIvV"}},{"cell_type":"code","source":"class MultiHeadAttentionBlock(nn.Module):\n\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.h = h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n\n        self.d_k = d_model // h\n        self.w_q = nn.Linear(d_model, d_model, bias=False)\n        self.w_k = nn.Linear(d_model, d_model, bias=False)\n        self.w_v = nn.Linear(d_model, d_model, bias=False)\n        self.w_o = nn.Linear(d_model, d_model, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    @staticmethod\n    def attention(query, key, value, mask, dropout: nn.Dropout):\n        d_k = query.shape[-1]\n        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            attention_scores.masked_fill_(mask == 0, -1e9)\n        attention_scores = attention_scores.softmax(dim=-1)\n        if dropout is not None:\n            attention_scores = dropout(attention_scores)\n        return (attention_scores @ value), attention_scores\n\n    def forward(self, q, k, v, mask):\n        query = self.w_q(q)\n        key = self.w_k(k)\n        value = self.w_v(v)\n\n        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n\n        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n\n        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n\n        return self.w_o(x)","metadata":{"id":"6ujcqPp1sOU9","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.738181Z","iopub.execute_input":"2025-01-19T14:22:54.738518Z","iopub.status.idle":"2025-01-19T14:22:54.755952Z","shell.execute_reply.started":"2025-01-19T14:22:54.738485Z","shell.execute_reply":"2025-01-19T14:22:54.755104Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Part 6: Residual Connection\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the architecture of the Transformer, we see that each sub-layer, including the <i>self-attention</i> and <i>Feed Forward</i> blocks, adds its output to its input before passing it to the <i>Add &amp; Norm</i> layer. This approach integrates the output with the original input in the <i>Add &amp; Norm</i> layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>ResidualConnection</code> class below is responsible for this process.</p>","metadata":{"id":"wCaLjCVxsWIc"}},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n\n        def __init__(self, features: int, dropout: float) -> None:\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            self.norm = LayerNormalization(features)\n\n        def forward(self, x, sublayer):\n            return x + self.dropout(sublayer(self.norm(x)))","metadata":{"id":"f-bvuGhIsdfu","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.756727Z","iopub.execute_input":"2025-01-19T14:22:54.756965Z","iopub.status.idle":"2025-01-19T14:22:54.771427Z","shell.execute_reply.started":"2025-01-19T14:22:54.756946Z","shell.execute_reply":"2025-01-19T14:22:54.770440Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Part 7: Encoder\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now build the encoder. We create the <code>EncoderBlock</code> class, consisting of the Multi-Head Attention and Feed Forward layers, plus the residual connections.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the Encoder Block repeats six times. We create the <code>Encoder</code> class as an assembly of multiple <code>EncoderBlock</code>s. We also add layer normalization as a final step after processing the input through all its blocks.</p>","metadata":{"id":"9YYI5vpasdGm"}},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n\n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x","metadata":{"id":"fRtppwE1s0-t","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.772319Z","iopub.execute_input":"2025-01-19T14:22:54.772628Z","iopub.status.idle":"2025-01-19T14:22:54.785592Z","shell.execute_reply.started":"2025-01-19T14:22:54.772599Z","shell.execute_reply":"2025-01-19T14:22:54.784701Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)","metadata":{"id":"eSq7BZWcs5s1","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.786432Z","iopub.execute_input":"2025-01-19T14:22:54.786927Z","iopub.status.idle":"2025-01-19T14:22:54.802960Z","shell.execute_reply.started":"2025-01-19T14:22:54.786898Z","shell.execute_reply":"2025-01-19T14:22:54.802061Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Part 8: Decoder\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Similarly, the Decoder also consists of several DecoderBlocks that repeat six times in the original paper. The main difference is that it has an additional sub-layer that performs multi-head attention with a <i>cross-attention</i> component that uses the output of the Encoder as its keys and values while using the Decoder's input as queries.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For the Output Embedding, we can use the same <code>InputEmbeddings</code> class we use for the Encoder. You can also notice that the self-attention sub-layer is <i>masked</i>, which restricts the model from accessing future elements in the sequence.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will start by building the <code>DecoderBlock</code> class, and then we will build the <code>Decoder</code> class, which will assemble multiple <code>DecoderBlock</code>s.</p>","metadata":{"id":"P0HXE1fH5g0W"}},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock,\n                 cross_attention_block: MultiHeadAttentionBlock,\n                 feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n        x = self.residual_connections[2](x, self.feed_forward_block)\n        return x","metadata":{"id":"V9Aof9mb4PJX","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.803933Z","iopub.execute_input":"2025-01-19T14:22:54.804230Z","iopub.status.idle":"2025-01-19T14:22:54.816811Z","shell.execute_reply.started":"2025-01-19T14:22:54.804209Z","shell.execute_reply":"2025-01-19T14:22:54.815920Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Decoder(nn.Module):\n\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)","metadata":{"id":"vwdthvkrtNUM","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.817622Z","iopub.execute_input":"2025-01-19T14:22:54.817930Z","iopub.status.idle":"2025-01-19T14:22:54.830561Z","shell.execute_reply.started":"2025-01-19T14:22:54.817902Z","shell.execute_reply":"2025-01-19T14:22:54.829884Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">You can see in the Decoder image that after running a stack of <code>DecoderBlock</code>s, we have a Linear Layer and a Softmax function to the output of probabilities. The <code>ProjectionLayer</code> class below is responsible for converting the output of the model into a probability distribution over the <i>vocabulary</i>, where we select each output token from a vocabulary of possible tokens.</p>","metadata":{"id":"Qm4g_8O1tS3d"}},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n\n    def __init__(self, d_model, vocab_size) -> None:\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x) -> None:\n        return self.proj(x)","metadata":{"id":"UbWVoNintThN","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.833425Z","iopub.execute_input":"2025-01-19T14:22:54.833638Z","iopub.status.idle":"2025-01-19T14:22:54.848201Z","shell.execute_reply.started":"2025-01-19T14:22:54.833620Z","shell.execute_reply":"2025-01-19T14:22:54.847316Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Part 9: Building the Transformer\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally have every component of the Transformer architecture ready. We may now construct the Transformer by putting it all together.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>Transformer</code> class below, we will bring together all the components of the model's architecture.</p>","metadata":{"id":"waCzPEAxtaR8"}},{"cell_type":"code","source":"class Transformer(nn.Module):\n\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings,\n                 tgt_embed: InputEmbeddings, src_pos: PositionalEncoding,\n                 tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer\n\n    def encode(self, src, src_mask):\n        src = self.src_embed(src)\n        src = self.src_pos(src)\n        return self.encoder(src, src_mask)\n\n    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n        tgt = self.tgt_embed(tgt)\n        tgt = self.tgt_pos(tgt)\n        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n\n    def project(self, x):\n        return self.projection_layer(x)","metadata":{"id":"qXbPW4oCtk2G","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.849425Z","iopub.execute_input":"2025-01-19T14:22:54.849682Z","iopub.status.idle":"2025-01-19T14:22:54.862385Z","shell.execute_reply.started":"2025-01-19T14:22:54.849650Z","shell.execute_reply":"2025-01-19T14:22:54.861407Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The architecture is finally ready. We now define a function called <code>build_transformer</code>, in which we define the parameters and everything we need to have a fully operational Transformer model for the task of <b>machine translation</b>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will set the same parameters as in the original paper, <a href = \"https://arxiv.org/pdf/1706.03762.pdf\"><i>Attention Is All You Need</i></a>, where $d_{model}$ = 512, $N$ = 6, $h$ = 8, dropout rate $P_{drop}$ = 0.1, and $d_{ff}$ = 2048.</p>","metadata":{"id":"6znypMaetmRk"}},{"cell_type":"code","source":"def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int,\n                      tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8,\n                      dropout: float=0.1, d_ff: int=2048) -> Transformer:\n    src_embed = InputEmbeddings(d_model, src_vocab_size)\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n\n    encoder_blocks = []\n    for _ in range(N):\n        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n        encoder_blocks.append(encoder_block)\n\n    decoder_blocks = []\n    for _ in range(N):\n        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n        decoder_blocks.append(decoder_block)\n\n    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n\n    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n\n    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n\n    for p in transformer.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n\n    return transformer","metadata":{"id":"bqGnJ6w2twJc","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.863433Z","iopub.execute_input":"2025-01-19T14:22:54.863717Z","iopub.status.idle":"2025-01-19T14:22:54.878467Z","shell.execute_reply.started":"2025-01-19T14:22:54.863686Z","shell.execute_reply":"2025-01-19T14:22:54.877511Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"The model is now ready to be trained!","metadata":{"id":"Iw7CWf4bt3yr"}},{"cell_type":"markdown","source":"## Part 10: Tokenizer","metadata":{"id":"6_7Z3fEYuTK0"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Tokenization is a crucial preprocessing step for our Transformer model. In this step, we convert raw text into a number format that the model can process.  </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">There are several Tokenization strategies. We will use the <i>word-level tokenization</i> to transform each word in a sentence into a token.</p>","metadata":{"id":"EDinqTghqr_Q"}},{"cell_type":"markdown","source":"<center>\n    <img src = \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5e749c-b0bd-4496-85a1-9b4397ad935f_1400x787.jpeg\" width = 800, height= 800>\n<p style = \"font-size: 16px;\n            font-family: 'Georgia', serif;\n            text-align: center;\n            margin-top: 10px;\">Different tokenization strategies. Source: <a href = \"https://shaankhosla.substack.com/p/talking-tokenization\">shaankhosla.substack.com</a>.</p>\n</center>","metadata":{"id":"at-cYYjnqr_Q"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">After tokenizing a sentence, we map each token to an unique integer ID based on the created vocabulary present in the training corpus during the training of the tokenizer. Each integer number represents a specific word in the vocabulary.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Besides the words in the training corpus, Transformers use special tokens for specific purposes. These are some that we will define right away:</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>• [UNK]:</b> This token is used to identify an unknown word in the sequence.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>• [PAD]:</b> Padding token to ensure that all sequences in a batch have the same length, so we pad shorter sentences with this token. We use attention masks to <i>\"tell\"</i> the model to ignore the padded tokens during training since they don't have any real meaning to the task.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>•  [SOS]:</b> This is a token used to signal the <i>Start of Sentence</i>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>•  [EOS]:</b> This is a token used to signal the <i>End of Sentence</i>.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>build_tokenizer</code> function below, we ensure a tokenizer is ready to train the model. It checks if there is an existing tokenizer, and if that is not the case, it trains a new tokenizer.</p>","metadata":{"id":"gjRMr2N6qr_Q"}},{"cell_type":"code","source":"def get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer","metadata":{"id":"Zh9pOItduxHq","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.879497Z","iopub.execute_input":"2025-01-19T14:22:54.879810Z","iopub.status.idle":"2025-01-19T14:22:54.894516Z","shell.execute_reply.started":"2025-01-19T14:22:54.879782Z","shell.execute_reply":"2025-01-19T14:22:54.893910Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Part 11: Load Dataset","metadata":{"id":"oodlr4eouxTU"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For this task, we will use the <a href = \"opus_books · Datasets at Hugging Face\">OpusBooks dataset</a>, available on 🤗Hugging Face. This dataset consists of two features, <code>id</code> and <code>translation</code>. The <code>translation</code> feature contains pairs of sentences in different languages, such as Spanish and Portuguese, English and French, and so forth.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I first tried translating sentences from English to Portuguese—my native tongue — but there are only 1.4k examples for this pair, so the results were not satisfying in the current configurations for this model. I then tried to use the English-French pair due to its higher number of examples—127k—but it would take too long to train with the current configurations. I then opted to train the model on the English-Italian pair, the same one used in the <a href = \"https://youtu.be/ISNdQcPhsts?si=253J39cose6IdsLv\">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference\n</a> video, as that was a good balance between performance and time of training.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by defining the <code>get_all_sentences</code> function to iterate over the dataset and extract the sentences according to the language pair defined—we will do that later.</p>","metadata":{"id":"YdVFowgUqr_Q"}},{"cell_type":"code","source":"def get_all_sentences(ds, lang):\n    for item in ds:\n        yield item['translation'][lang]","metadata":{"id":"xvRuuTpIveZS","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.895274Z","iopub.execute_input":"2025-01-19T14:22:54.895594Z","iopub.status.idle":"2025-01-19T14:22:54.910379Z","shell.execute_reply.started":"2025-01-19T14:22:54.895564Z","shell.execute_reply":"2025-01-19T14:22:54.909637Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_ds</code> function is defined to load and prepare the dataset for training and validation. In this function, we build or load the tokenizer, split the dataset, and create DataLoaders, so the model can successfully iterate over the dataset in batches. The result of these functions is tokenizers for the source and target languages plus the DataLoader objects.</p>","metadata":{"id":"EA13IRYEqr_R"}},{"cell_type":"code","source":"def get_ds(config):\n\n    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n\n    max_len_src = 0\n    max_len_tgt = 0\n\n    for item in ds_raw:\n        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n    print(f'Max length of source sentence: {max_len_src}')\n    print(f'Max length of target sentence: {max_len_tgt}')\n\n\n    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt","metadata":{"id":"IkTRqP8LvpVy","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.911166Z","iopub.execute_input":"2025-01-19T14:22:54.911430Z","iopub.status.idle":"2025-01-19T14:22:54.926614Z","shell.execute_reply.started":"2025-01-19T14:22:54.911366Z","shell.execute_reply":"2025-01-19T14:22:54.925719Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We define the <code>casual_mask</code> function to create a mask for the attention mechanism of the decoder. This mask prevents the model from having information about future elements in the sequence. </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by making a square grid filled with ones. We determine the grid size with the <code>size</code> parameter. Then, we change all the numbers above the main diagonal line to zeros. Every number on one side becomes a zero, while the rest remain ones. The function then flips all these values, turning ones into zeros and zeros into ones. This process is crucial for models that predict future tokens in a sequence.</p>","metadata":{"id":"VK3d2-AVqr_R"}},{"cell_type":"code","source":"def causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0","metadata":{"id":"kTgMYaY2vvWq","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.927544Z","iopub.execute_input":"2025-01-19T14:22:54.927870Z","iopub.status.idle":"2025-01-19T14:22:54.942748Z","shell.execute_reply.started":"2025-01-19T14:22:54.927820Z","shell.execute_reply":"2025-01-19T14:22:54.942116Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>BilingualDataset</code> class processes the texts of the target and source languages in the dataset by tokenizing them and adding all the necessary special tokens. This class also certifies that the sentences are within a maximum sequence length for both languages and pads all necessary sentences.</p>","metadata":{"id":"ccdK5XnMqr_R"}},{"cell_type":"code","source":"class BilingualDataset(Dataset):\n\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n\n    def __len__(self):\n        return len(self.ds)\n\n    def __getitem__(self, idx):\n        src_target_pair = self.ds[idx]\n        src_text = src_target_pair['translation'][self.src_lang]\n        tgt_text = src_target_pair['translation'][self.tgt_lang]\n\n        # Transform the text into tokens\n        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n\n        # Add sos, eos and padding to each sentence\n        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n        # We will only add <s>, and </s> only on the label\n        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n\n        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n            raise ValueError(\"Sentence is too long\")\n\n        # Add <s> and </s> token\n        encoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(enc_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n        # Add only <s> token\n        decoder_input = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n        # Add only </s> token\n        label = torch.cat(\n            [\n                torch.tensor(dec_input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n\n        # Double check the size of the tensors to make sure they are all seq_len long\n        assert encoder_input.size(0) == self.seq_len\n        assert decoder_input.size(0) == self.seq_len\n        assert label.size(0) == self.seq_len\n\n        return {\n            \"encoder_input\": encoder_input,  # (seq_len)\n            \"decoder_input\": decoder_input,  # (seq_len)\n            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n            \"label\": label,  # (seq_len)\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text,\n        }","metadata":{"id":"x9v94mdgv3y6","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.943612Z","iopub.execute_input":"2025-01-19T14:22:54.943933Z","iopub.status.idle":"2025-01-19T14:22:54.957786Z","shell.execute_reply.started":"2025-01-19T14:22:54.943902Z","shell.execute_reply":"2025-01-19T14:22:54.957022Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Part 12: Validation Loop","metadata":{"id":"B7cXlNUfv5uL"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now create two functions for the validation loop. The validation loop is crucial to evaluate model performance in translating sentences from data it has not seen during training.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will define two functions. The first function, <code>greedy_decode</code>, gives us the model's output by obtaining the most probable next token. The second function, <code>run_validation</code>, is responsible for running the validation process in which we decode the model's output and compare it with the reference text for the target sentence.</p>","metadata":{"id":"tf8Wt860qr_R"}},{"cell_type":"code","source":"def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n\n    encoder_output = model.encode(source, source_mask)\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n    while True:\n        if decoder_input.size(1) == max_len:\n            break\n\n        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n\n        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n        prob = model.project(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        decoder_input = torch.cat(\n            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n        )\n\n        if next_word == eos_idx:\n            break\n\n    return decoder_input.squeeze(0)","metadata":{"id":"z1rzcAkpv8Ew","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.958562Z","iopub.execute_input":"2025-01-19T14:22:54.958820Z","iopub.status.idle":"2025-01-19T14:22:54.974823Z","shell.execute_reply.started":"2025-01-19T14:22:54.958799Z","shell.execute_reply":"2025-01-19T14:22:54.974124Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n    model.eval()\n    count = 0\n\n    source_texts = []\n    expected = []\n    predicted = []\n\n    try:\n        with os.popen('stty size', 'r') as console:\n            _, console_width = console.read().split()\n            console_width = int(console_width)\n    except:\n        console_width = 80\n\n    with torch.no_grad():\n        for batch in validation_ds:\n            count += 1\n            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n\n            assert encoder_input.size(\n                0) == 1, \"Batch size must be 1 for validation\"\n\n            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n\n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n            source_texts.append(source_text)\n            expected.append(target_text)\n            predicted.append(model_out_text)\n\n\n            print_msg('-'*console_width)\n            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n\n            if count == num_examples:\n                print_msg('-'*console_width)\n                break\n\n    if writer:\n        metric = torchmetrics.CharErrorRate()\n        cer = metric(predicted, expected)\n        writer.add_scalar('validation cer', cer, global_step)\n        writer.flush()\n\n        metric = torchmetrics.WordErrorRate()\n        wer = metric(predicted, expected)\n        writer.add_scalar('validation wer', wer, global_step)\n        writer.flush()\n\n        # Compute the BLEU metric\n        metric = torchmetrics.BLEUScore()\n        bleu = metric(predicted, expected)\n        writer.add_scalar('validation BLEU', bleu, global_step)\n        writer.flush()","metadata":{"id":"iF7v9L0owcLT","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:54.975570Z","iopub.execute_input":"2025-01-19T14:22:54.975904Z","iopub.status.idle":"2025-01-19T14:22:54.987864Z","shell.execute_reply.started":"2025-01-19T14:22:54.975882Z","shell.execute_reply":"2025-01-19T14:22:54.987208Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Part 13: Training Loop","metadata":{"id":"qw3nykKxwkIh"}},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We are ready to train our Transformer model on the OpusBook dataset for the English to Italian translation task.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We first start by defining the <code>get_model</code> function to load the model by calling the <code>build_transformer</code> function we have previously defined. This function uses the <code>config</code> dictionary to set a few parameters.</p>","metadata":{"id":"az_Kwq4Zqr_S"}},{"cell_type":"code","source":"def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n    return model","metadata":{"id":"7QMn1BULwnBl","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:57.175121Z","iopub.execute_input":"2025-01-19T14:22:57.175407Z","iopub.status.idle":"2025-01-19T14:22:57.179332Z","shell.execute_reply.started":"2025-01-19T14:22:57.175387Z","shell.execute_reply":"2025-01-19T14:22:57.178393Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I have mentioned the <code>config</code> dictionary several times throughout this notebook. Now, it is time to create it.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the following cell, we will define two functions to configure our model and the training process.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>get_config</code> function, we define crucial parameters for the training process. <code>batch_size</code> for the number of training examples used in one iteration, <code>num_epochs</code> as the number of times the entire dataset is passed forward and backward through the Transformer, <code>lr</code> as the learning rate for the optimizer, etc. We will also finally define the pairs from the OpusBook dataset, <code>'lang_src': 'en'</code> for selecting English as the source language and <code>'lang_tgt': 'it'</code> for selecting Italian as the target language.</p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_weights_file_path</code> function constructs the file path for saving or loading model weights for any specific epoch.</p>","metadata":{"id":"Ord2DlVkqr_S"}},{"cell_type":"code","source":"def get_config():\n    return {\n        \"batch_size\": 8,\n        \"num_epochs\": 8,\n        \"lr\": 10**-4,\n        \"seq_len\": 350,\n        \"d_model\": 512,\n        \"datasource\": 'opus_books',\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"it\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": \"latest\",\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"experiment_name\": \"runs/tmodel\"\n    }\n\ndef get_weights_file_path(config, epoch: str):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n    return str(Path('.') / model_folder / model_filename)\n\n# Find the latest weights file in the weights folder\ndef latest_weights_file_path(config):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    if len(weights_files) == 0:\n        return None\n    weights_files.sort()\n    return str(weights_files[-1])","metadata":{"id":"gXt82CejxeHZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:22:58.851110Z","iopub.execute_input":"2025-01-19T14:22:58.851425Z","iopub.status.idle":"2025-01-19T14:22:58.857075Z","shell.execute_reply.started":"2025-01-19T14:22:58.851399Z","shell.execute_reply":"2025-01-19T14:22:58.856207Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally define our last function, <code>train_model</code>, which takes the <code>config</code> arguments as input. </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In this function, we will set everything up for the training. We will load the model and its necessary components onto the GPU for faster training, set the <code>Adam</code> optimizer, and configure the <code>CrossEntropyLoss</code> function to compute the differences between the translations output by the model and the reference translations from the dataset. </p>\n\n<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Every loop necessary for iterating over the training batches, performing backpropagation, and computing the gradients is in this function. We will also use it to run the validation function and save the current state of the model.</p>","metadata":{"id":"Qw7SjmrDqr_S"}},{"cell_type":"code","source":"def train_model(config):\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n    if (device == 'cuda'):\n        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n    elif (device == 'mps'):\n        print(f\"Device name: <mps>\")\n    else:\n        print(\"NOTE: If you have a GPU, consider using it for training.\")\n        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n    device = torch.device(device)\n\n    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n\n    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n    # Tensorboard\n    writer = SummaryWriter(config['experiment_name'])\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n\n    initial_epoch = 0\n    global_step = 0\n    preload = config['preload']\n    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n    if model_filename:\n        print(f'Preloading model {model_filename}')\n        state = torch.load(model_filename)\n        model.load_state_dict(state['model_state_dict'])\n        initial_epoch = state['epoch'] + 1\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        global_step = state['global_step']\n    else:\n        print('No model to preload, starting from scratch')\n\n    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n    for epoch in range(initial_epoch, config['num_epochs']):\n        torch.cuda.empty_cache()\n        model.train()\n        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n        for batch in batch_iterator:\n\n            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n\n            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n\n            label = batch['label'].to(device) # (B, seq_len)\n\n            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n            writer.add_scalar('train loss', loss.item(), global_step)\n            writer.flush()\n\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n\n            global_step += 1\n\n        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n\n        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'global_step': global_step\n        }, model_filename)","metadata":{"id":"2qK9wAjRxoDQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:23:00.581173Z","iopub.execute_input":"2025-01-19T14:23:00.581469Z","iopub.status.idle":"2025-01-19T14:23:00.591456Z","shell.execute_reply.started":"2025-01-19T14:23:00.581442Z","shell.execute_reply":"2025-01-19T14:23:00.590716Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"We can now train the model!","metadata":{"id":"nrMmfyi8xrXw"}},{"cell_type":"code","source":"if __name__ == '__main__':\n    warnings.filterwarnings(\"ignore\")\n    config = get_config()\n    train_model(config)","metadata":{"id":"28425EYaxrsi","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:23:03.570909Z","iopub.execute_input":"2025-01-19T14:23:03.571213Z","iopub.status.idle":"2025-01-19T16:23:47.344337Z","shell.execute_reply.started":"2025-01-19T14:23:03.571191Z","shell.execute_reply":"2025-01-19T16:23:47.343628Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDevice name: Tesla P100-PCIE-16GB\nDevice memory: 15.887939453125 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ba3ca5c93a47849a0e3c6b7b2f4f3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5660393b96242ad965d006e1591a060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993caab9cccd41199cb33296ffa35c54"}},"metadata":{}},{"name":"stdout","text":"Max length of source sentence: 309\nMax length of target sentence: 274\nNo model to preload, starting from scratch\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 3638/3638 [15:00<00:00,  4.04it/s, loss=5.271]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: It was too far to return to dinner, and an allowance of cold meat and bread, in the same penurious proportion observed in our ordinary meals, was served round between the services.\n    TARGET: Era troppo lontano per tornare a pranzo, così fra i due servizi ci davano pane e carne fredda, in porzioni insufficienti come al solito.\n PREDICTED: Il mio momento era un ’ altra , e si , e , e la sua volta , e la sua volta , e la sua vita , e la sua vita .\n--------------------------------------------------------------------------------\n    SOURCE: Levin was just preparing to start a conversation with the old waiter when the Secretary of the Court of Nobility, an old man whose speciality it was to know all the nobles of the Province by name and patronymic, diverted his attention.\n    TARGET: Levin stava proprio per intavolare una conversazione col vecchio cameriere, quando il segretario della tutela nobiliare, un vecchietto che aveva la specialità di conoscere tutti i nobili del governatorato per nome e patronimico, lo distrasse.\n PREDICTED: Levin era un ’ ka , e la sua volta , e la sua volta , e la sua volta , e la sua volta , e la sua volta , e la sua vita .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 3638/3638 [15:02<00:00,  4.03it/s, loss=5.144]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: Well, yes!'\n    TARGET: È così.\n PREDICTED: È vero !\n--------------------------------------------------------------------------------\n    SOURCE: He must not find me with red eyes.\n    TARGET: Bisogna che non mi veda con gli occhi rossi di pianto.\n PREDICTED: Non mi , signore .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 3638/3638 [15:02<00:00,  4.03it/s, loss=5.591]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: Boil it up a little longer, Agatha Mikhaylovna.'\n    TARGET: Cuocete ancora, Agaf’ja Michajlovna.\n PREDICTED: un ’ altra parte , Stepan Arkad ’ ic .\n--------------------------------------------------------------------------------\n    SOURCE: We had intended to push on to Wallingford that day, but the sweet smiling face of the river here lured us to linger for a while; and so we left our boat at the bridge, and went up into Streatley, and lunched at the \"Bull,\" much to Montmorency's satisfaction.\n    TARGET: Avevamo determinato di spingerci fino a Wallingford quel giorno; ma, la dolce sorridente faccia del fiume in quel punto ci persuase a sostare un po’; e così lasciammo la nostra barca presso il ponte, ed entrammo in Streatley, e facemmo colazione al «Toro» con gran soddisfazione di Montmorency.\n PREDICTED: a il bosco , ma , dopo aver fatto la testa , ma , dopo aver fatto la testa , e ci , e ci a di , e , e di , e di .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03: 100%|██████████| 3638/3638 [15:02<00:00,  4.03it/s, loss=4.988]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: I don't think I have a jealous nature.\n    TARGET: Io penso di non essere gelosa. Non sono gelosa.\n PREDICTED: Non so che non posso .\n--------------------------------------------------------------------------------\n    SOURCE: After long efforts to get an injunction removed the money was all ready to be paid out; but the notary – a very obliging man – could not give the warrant because it needed the President's signature, and the President was engaged at the Session and had not appointed a substitute.\n    TARGET: Dopo lunghe sollecitazioni perché fosse tolto il divieto, il denaro era pronto per il pagamento; ma il notaio, uomo servizievolissimo, non poteva consegnare il mandato, perché era necessaria la firma del presidente, e il presidente, senza aver fatto la consegna dell'ufficio, era alla sessione.\n PREDICTED: Dopo aver fatto un po ’ di , ma in cui era stato stato stato da fare , ma non c ’ era nulla di , né , né , né , né la di un uomo , e la .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 04: 100%|██████████| 3638/3638 [15:01<00:00,  4.04it/s, loss=4.722]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: You are not to suppose, reader, that Adele has all this time been sitting motionless on the stool at my feet: no; when the ladies entered, she rose, advanced to meet them, made a stately reverence, and said with gravity-- \"Bon jour, mesdames.\"\n    TARGET: Appena le signore furono entrate, Adele andò loro incontro, e, salutandole con cerimonia, aveva detto: — Buon giorno, signore.\n PREDICTED: Non siete stata la mia casa , che la mia Adele è stata rossa , e non si mise a guardare il mio vestito , quando si mise a , e si mise a , e si mise a .\n--------------------------------------------------------------------------------\n    SOURCE: She was looking about for some way of escape, and wondering whether she could get away without being seen, when she noticed a curious appearance in the air: it puzzled her very much at first, but, after watching it a minute or two, she made it out to be a grin, and she said to herself 'It's the Cheshire Cat: now I shall have somebody to talk to.'\n    TARGET: E pensava di svignarsela, quando scorse uno strano spettacolo in aria. Prima ne restò sorpresa, ma dopo aver guardato qualche istante, vide un ghigno e disse fra sè: “È Ghignagatto: potrò finalmente parlare con qualcuno.”\n PREDICTED: Ella aveva cominciato a pensare a un po ’ di nuovo , e , senza pensare se stesso , senza dubbio , si sentiva in un altro momento , e quando si sentiva in un altro momento , dopo un altro momento , e si mise a , e disse : — È un pezzo di , — disse il signor Rochester , — e ci sono molto contento di .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 05: 100%|██████████| 3638/3638 [15:01<00:00,  4.03it/s, loss=3.924]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: This answer had seemed horrible to Dolly, despite the good-natured sweetness of the young woman's looks, but now she could not help recalling it.\n    TARGET: Questa risposta era parsa ripugnante a Dar’ja Aleksandrovna, malgrado l’aspetto buono della giovane donna; ma ora ella si ricordò involontariamente di quelle parole.\n PREDICTED: Questa risposta aveva detto a Dar ’ ja Aleksandrovna , ma con l ’ aria particolare di gioia , ma non poteva capire nulla , ma non poteva capire nulla .\n--------------------------------------------------------------------------------\n    SOURCE: I could not help laughing, he looked such a ridiculous figure.\n    TARGET: Non potei non ridere dinanzi a una figura così ridicola.\n PREDICTED: Non potevo capire , guardando con un ’ espressione di gioia .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 06: 100%|██████████| 3638/3638 [15:01<00:00,  4.04it/s, loss=4.319]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: I meant to compromise matters by going down to the edge and just throwing the water over myself; so I took a towel and crept out on the bank and wormed my way along on to the branch of a tree that dipped down into the water.\n    TARGET: Intendevo d’aggiustar la faccenda con l’andare sulla proda e gettarmi dell’acqua addosso, così mi presi un accappatoio e andai innanzi strisciando su un ramo d’albero che si tuffava nell’acqua.\n PREDICTED: Io volevo a contro il ginocchio e sulla riva , e mi misi a un po ’ di carta , e mi misi a camminare sulla riva , a un albero che mi portò in mezzo alla riva .\n--------------------------------------------------------------------------------\n    SOURCE: \"You, madam,\" said he, \"are cleared from all blame: your uncle will be glad to hear it--if, indeed, he should be still living--when Mr. Mason returns to Madeira.\"\n    TARGET: — Voi, signora, — diss'egli, — siete innocente e vostro zio sarà ben felice di saperlo, se pure vivrà ancora al ritorno del signor Mason a Madera.\n PREDICTED: — Siete contenta , — disse , — e voi siete contenta di voi , se è contenta di esser contenta di esser più di voi , quando il signor Mason , la signorina Temple si ha detto a prendere il signor Mason .\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 07: 100%|██████████| 3638/3638 [15:03<00:00,  4.03it/s, loss=4.020]\n","output_type":"stream"},{"name":"stdout","text":"--------------------------------------------------------------------------------\n    SOURCE: At the Marshal's table, beneath the portrait of the Emperor, discussions were in full swing.\n    TARGET: Alla tavola del governatorato, sotto il ritratto dello zar, si svolgevano i dibattiti.\n PREDICTED: Alla vettura , in cui , in un ritratto , erano in piedi , erano in piedi .\n--------------------------------------------------------------------------------\n    SOURCE: And now—to-night—Ethelred—ha! ha!—the breaking of the hermit’s door, and the death-cry of the dragon, and the clangor of the shield!—say, rather, the rending of her coffin, and the grating of the iron hinges of her prison, and her struggles within the coppered archway of the vault!\n    TARGET: Ed ora – stanotte – Etelredo, ah! ah! la porta dell'eremita sfondata, e il rantolo del dragone, e il fragore dello scudo! Dite piuttosto l'infrangersi della sua bara, e lo stridere dei cardini di ferro della sua prigione, e la sua lotta spaventevole nel vestibolo di rame!\n PREDICTED: E ora , la notte , – , – , , , la camera del bambino e la morte del suo padrone , e la , il , il , il , il , il , il , il , il e il grido si .\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Section 2: BERT and LoRA\n\nWelcome to Section 2 of our Machine Learning assignment! I hope you've been enjoying the journey so far! 😊\n\n In this section, you will gain hands-on experience with [BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) and [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation) for text classification tasks. The section is divided into three main parts, each focusing on different aspects of NLP techniques.\n\n## Assignment Structure\n\n### Part 1: Data Preparation and Preprocessing\nIn this part, you will work with a text classification dataset. You will learn how to:\n- Download and load the dataset\n- Perform necessary preprocessing steps\n- Implement data cleaning and transformation techniques\n- Prepare the data in a format suitable for BERT training\n\n### Part 2: Building a Small BERT Model\nYou will create and train a small BERT model from scratch using the Hugging Face [Transformers](https://huggingface.co/docs/transformers/en/index) library. This part will help you understand:\n- The architecture of BERT\n- How to configure and initialize a BERT model\n- Training process and optimization\n- Model evaluation and performance analysis\n\n### Part 3: Fine-tuning with LoRA\nIn the final part, you will work with a pre-trained [TinyBERT](https://arxiv.org/abs/1909.10351) model and use LoRA for efficient fine-tuning. You will:\n- Load a pre-trained TinyBERT model\n- Implement LoRA adaptation and fine-tune the model on our classification task\n- Compare the results with the previous approach","metadata":{"id":"v3axMN7QWiVH"}},{"cell_type":"markdown","source":"---\n\n> **NOTE**:  \n> Throughout this notebook, make an effort to include sufficient visualizations to enhance understanding:  \n> - In the data processing section, display the results of your operations (e.g., show data samples or distributions after preprocessing).  \n> - In the classification section, report various evaluation metrics such as accuracy, precision, recall, and F1-score to thoroughly assess your model's performance.  \n> - Additionally, take a moment to compare the sizes of the models discussed in this notebook with today’s enormous models. This will help you appreciate the challenges and computational demands associated with training such massive models. 😵‍💫\n\n---\n","metadata":{"id":"M6FKcSFbOTMd"}},{"cell_type":"markdown","source":"## Part 1: Data Preparation and Preprocessing\nWe'll be working with the [Consumer Complaint](https://catalog.data.gov/dataset/consumer-complaint-database) dataset, which contains ***complaints*** submitted by consumers about financial products and services. Our goal is to build a classifier that can automatically identify the type of complaint based on the consumer's text description. For this task, we will work with a smaller subset of the dataset, available for download through this [link](https://drive.google.com/file/d/1SpIHksR-WzruEgUjp1SQKGG8bZPnJJoN/view?usp=sharing).","metadata":{"id":"GHKw2r6yYV7n"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gdown\nimport zipfile\nimport re\nfrom bs4 import BeautifulSoup\n","metadata":{"id":"7ELMR8kXUh3o","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:02.072072Z","iopub.execute_input":"2025-01-09T12:02:02.072355Z","iopub.status.idle":"2025-01-09T12:02:02.077278Z","shell.execute_reply.started":"2025-01-09T12:02:02.072332Z","shell.execute_reply":"2025-01-09T12:02:02.076284Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 1.2 Loading the Data","metadata":{"id":"9oJXlKLYeymq"}},{"cell_type":"code","source":"url = \"https://drive.google.com/uc?id=10GRrnrKH0JTCLVTCdleg9rx3GvdbFYYj\"\noutput = \"dataset.csv\"\ngdown.download(url, output, quiet=False)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"id":"yWdMEFBsn6wO","outputId":"3dbc736e-07d4-431a-9469-7da4e803aeca","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:02.078566Z","iopub.execute_input":"2025-01-09T12:02:02.078876Z","iopub.status.idle":"2025-01-09T12:02:14.631404Z","shell.execute_reply.started":"2025-01-09T12:02:02.078845Z","shell.execute_reply":"2025-01-09T12:02:14.630698Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=10GRrnrKH0JTCLVTCdleg9rx3GvdbFYYj\nFrom (redirected): https://drive.google.com/uc?id=10GRrnrKH0JTCLVTCdleg9rx3GvdbFYYj&confirm=t&uuid=5516dec8-99f2-4bc0-9f06-f6d875a730a6\nTo: /kaggle/working/dataset.csv\n100%|██████████| 999M/999M [00:07<00:00, 135MB/s]  \n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'dataset.csv'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"path = '/kaggle/working/dataset.csv'\ndf = pd.read_csv(path)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:26.063094Z","iopub.execute_input":"2025-01-09T12:02:26.063382Z","iopub.status.idle":"2025-01-09T12:02:35.056713Z","shell.execute_reply.started":"2025-01-09T12:02:26.063361Z","shell.execute_reply":"2025-01-09T12:02:35.055999Z"},"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"wQgwFzTg2VBY","outputId":"870007ef-daa3-493f-87e3-bb9556e5e896"},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                             Product  \\\n0  Credit reporting, credit repair services, or o...   \n1                                       Student loan   \n2  Credit reporting or other personal consumer re...   \n3  Credit reporting, credit repair services, or o...   \n4  Credit reporting or other personal consumer re...   \n\n                        Consumer complaint narrative  \n0  My credit reports are inaccurate. These inaccu...  \n1  Beginning in XX/XX/XXXX I had taken out studen...  \n2  I am disputing a charge-off on my account that...  \n3  I did not consent to, authorize, nor benefit f...  \n4  I am a federally protected consumer and I am a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Consumer complaint narrative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Credit reporting, credit repair services, or o...</td>\n      <td>My credit reports are inaccurate. These inaccu...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Student loan</td>\n      <td>Beginning in XX/XX/XXXX I had taken out studen...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Credit reporting or other personal consumer re...</td>\n      <td>I am disputing a charge-off on my account that...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Credit reporting, credit repair services, or o...</td>\n      <td>I did not consent to, authorize, nor benefit f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Credit reporting or other personal consumer re...</td>\n      <td>I am a federally protected consumer and I am a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### 1.3 Data Sampling and Class Distribution Analysis\n\nWorking with large datasets can be computationally intensive during development. Additionally, imbalanced class distribution can affect model performance. In this section, you'll sample the data and analyze class distributions to make informed decisions about your training dataset.","metadata":{"id":"L9hr8-FNgpVO"}},{"cell_type":"markdown","source":"---\n\nWe'll work with a manageable portion of the data to develop and test our approach. While using the complete dataset would likely yield better results, a smaller sample allows us to prototype our solution more efficiently.\n","metadata":{"id":"Cl_g_ZU4h5RG"}},{"cell_type":"code","source":"sampled_df = df.sample(frac=0.2, random_state=42)  # Adjust fraction as needed\n\nprint(f\"Sampled dataset shape: {sampled_df.shape}\")\n\nprint(\"Sampled dataset preview:\")\nsampled_df.head()","metadata":{"id":"QAJUXNCFhYsf","colab":{"base_uri":"https://localhost:8080/","height":242},"outputId":"f9b1780c-1757-4851-ae88-248438d36762","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:35.057621Z","iopub.execute_input":"2025-01-09T12:02:35.057902Z","iopub.status.idle":"2025-01-09T12:02:35.141009Z","shell.execute_reply.started":"2025-01-09T12:02:35.057879Z","shell.execute_reply":"2025-01-09T12:02:35.140339Z"}},"outputs":[{"name":"stdout","text":"Sampled dataset shape: (188226, 2)\nSampled dataset preview:\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                  Product  \\\n335123  Credit reporting or other personal consumer re...   \n601718                                           Mortgage   \n847752  Credit reporting, credit repair services, or o...   \n765316  Credit reporting or other personal consumer re...   \n798300  Credit reporting, credit repair services, or o...   \n\n                             Consumer complaint narrative  \n335123  Upon reviewing my credit report, I have identi...  \n601718  I was doing a rate check to refinance. The age...  \n847752  This is my 2nd request that I have been a vict...  \n765316  I'm sending this compliant to inform credit bu...  \n798300  Im submitting a complaint to you today to info...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Consumer complaint narrative</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>335123</th>\n      <td>Credit reporting or other personal consumer re...</td>\n      <td>Upon reviewing my credit report, I have identi...</td>\n    </tr>\n    <tr>\n      <th>601718</th>\n      <td>Mortgage</td>\n      <td>I was doing a rate check to refinance. The age...</td>\n    </tr>\n    <tr>\n      <th>847752</th>\n      <td>Credit reporting, credit repair services, or o...</td>\n      <td>This is my 2nd request that I have been a vict...</td>\n    </tr>\n    <tr>\n      <th>765316</th>\n      <td>Credit reporting or other personal consumer re...</td>\n      <td>I'm sending this compliant to inform credit bu...</td>\n    </tr>\n    <tr>\n      <th>798300</th>\n      <td>Credit reporting, credit repair services, or o...</td>\n      <td>Im submitting a complaint to you today to info...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"---\n\nLet's examine the distribution of ***complaints*** types in our dataset. You'll notice that some products have significantly more instances than others, and some categories are quite similar. For example:\n\n- Multiple categories might refer to similar financial products\n- Some categories might have very few examples\n- Certain categories might be subcategories of others\n\nYou have two main approaches to handle this situation:\n\n1. **Merging Similar Classes:** Identify categories that represent similar products/services and Combine them to create more robust, general categories\n\n2. **Selecting Major Classes:** Only select the categories with sufficient representation\n\n\n\n> You may choose any approach, but after this step, your data must include **at least five** distinct classes.\n\n","metadata":{"id":"50a4NJeMiBb6"}},{"cell_type":"code","source":"product_counts = df['Product'].value_counts()\nprint(\"Number of complaints per product category:\")\nprint(product_counts)","metadata":{"id":"nby2Hrwwjd46","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1ec382b5-fe8f-41e3-b578-5c74689fc32c","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:35.142706Z","iopub.execute_input":"2025-01-09T12:02:35.142965Z","iopub.status.idle":"2025-01-09T12:02:35.216005Z","shell.execute_reply.started":"2025-01-09T12:02:35.142944Z","shell.execute_reply":"2025-01-09T12:02:35.215091Z"}},"outputs":[{"name":"stdout","text":"Number of complaints per product category:\nProduct\nCredit reporting, credit repair services, or other personal consumer reports    322966\nCredit reporting or other personal consumer reports                             252019\nDebt collection                                                                 117285\nMortgage                                                                         49358\nChecking or savings account                                                      44580\nCredit card or prepaid card                                                      43575\nCredit card                                                                      24776\nStudent loan                                                                     18742\nMoney transfer, virtual currency, or money service                               17962\nVehicle loan or lease                                                            13777\nCredit reporting                                                                 12641\nPayday loan, title loan, or personal loan                                         6940\nBank account or service                                                           5920\nConsumer Loan                                                                     3881\nPrepaid card                                                                      2402\nPayday loan, title loan, personal loan, or advance loan                           2300\nPayday loan                                                                        723\nDebt or credit management                                                          593\nMoney transfers                                                                    571\nOther financial service                                                            110\nVirtual currency                                                                     7\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"threshold = 1000\nunder_represented = product_counts[product_counts < threshold]\nprint(\"\\nUnder-represented categories (less than {} examples):\".format(threshold))\nprint(under_represented)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZW2Nlk46wc7I","outputId":"e2d3d1c2-1823-46ee-bd02-81286a039b61","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:35.217496Z","iopub.execute_input":"2025-01-09T12:02:35.217825Z","iopub.status.idle":"2025-01-09T12:02:35.226435Z","shell.execute_reply.started":"2025-01-09T12:02:35.217790Z","shell.execute_reply":"2025-01-09T12:02:35.225582Z"}},"outputs":[{"name":"stdout","text":"\nUnder-represented categories (less than 1000 examples):\nProduct\nPayday loan                  723\nDebt or credit management    593\nMoney transfers              571\nOther financial service      110\nVirtual currency               7\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"merged_df = df.copy()\nmerged_df['Product'] = merged_df['Product'].replace({\n    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Reporting',\n    'Credit reporting or other personal consumer reports': 'Credit Reporting',\n    'Credit reporting': 'Credit Reporting',\n    'Credit card or prepaid card': 'Credit Card',\n    'Credit card': 'Credit Card',\n    'Payday loan, title loan, or personal loan': 'Payday Loan',\n    'Payday loan, title loan, personal loan, or advance loan': 'Payday Loan',\n    'Payday loan': 'Payday Loan',\n    'Debt or credit management': 'Debt Collection',\n    'Debt collection': 'Debt Collection',\n    'Mortgage': 'Home Loan',\n    'Vehicle loan or lease': 'Vehicle Loan',\n    'Checking or savings account': 'Bank Account',\n    'Bank account or service': 'Bank Account',\n    'Prepaid card': 'Prepaid Card',\n    'Money transfer, virtual currency, or money service': 'Money Transfers',\n    'Money transfers': 'Money Transfers',\n    'Virtual currency': 'Money Transfers',\n    'Student loan': 'Student Loan',\n    'Consumer Loan': 'Consumer Loan',\n    'Other financial service': 'Other Financial Service'\n})","metadata":{"id":"OVeFIwylwiKQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:35.227304Z","iopub.execute_input":"2025-01-09T12:02:35.227669Z","iopub.status.idle":"2025-01-09T12:02:35.849054Z","shell.execute_reply.started":"2025-01-09T12:02:35.227635Z","shell.execute_reply":"2025-01-09T12:02:35.848384Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"\\nUpdated category counts after merging:\")\nprint(merged_df['Product'].value_counts())\n\nif len(merged_df['Product'].unique()) >= 5:\n    print(\"\\nThe dataset includes at least 5 distinct classes after merging.\")\n\nmajor_classes_df = merged_df[merged_df['Product'].isin(merged_df['Product'].value_counts()[merged_df['Product'].value_counts() >= threshold].index)]\n\nprint(\"\\nCategory counts after filtering major classes:\")\nprint(major_classes_df['Product'].value_counts())\n\nif len(major_classes_df['Product'].unique()) >= 5:\n    print(\"\\nThe dataset includes at least 5 distinct classes after filtering.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kb36yB2kwogv","outputId":"3423c992-e64a-4553-ffbe-8ecc31ba055c","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:35.849809Z","iopub.execute_input":"2025-01-09T12:02:35.850019Z","iopub.status.idle":"2025-01-09T12:02:36.175395Z","shell.execute_reply.started":"2025-01-09T12:02:35.850001Z","shell.execute_reply":"2025-01-09T12:02:36.174724Z"}},"outputs":[{"name":"stdout","text":"\nUpdated category counts after merging:\nProduct\nCredit Reporting           587626\nDebt Collection            117878\nCredit Card                 68351\nBank Account                50500\nHome Loan                   49358\nStudent Loan                18742\nMoney Transfers             18540\nVehicle Loan                13777\nPayday Loan                  9963\nConsumer Loan                3881\nPrepaid Card                 2402\nOther Financial Service       110\nName: count, dtype: int64\n\nThe dataset includes at least 5 distinct classes after merging.\n\nCategory counts after filtering major classes:\nProduct\nCredit Reporting    587626\nDebt Collection     117878\nCredit Card          68351\nBank Account         50500\nHome Loan            49358\nStudent Loan         18742\nMoney Transfers      18540\nVehicle Loan         13777\nPayday Loan           9963\nConsumer Loan         3881\nPrepaid Card          2402\nName: count, dtype: int64\n\nThe dataset includes at least 5 distinct classes after filtering.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"---\n### 1.4 Data Encoding and Text Preprocessing\n\nBefore training our model, we need to prepare both our target labels and text data. This involves converting categorical labels into numerical format and cleaning our text data to improve model performance.","metadata":{"id":"lD3oISsijt1P"}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nmerged_df['Label'] = label_encoder.fit_transform(merged_df['Product'])\nprint(\"Label encoding complete. Here are the first few encoded labels:\")\nmerged_df[['Product', 'Label']].head()","metadata":{"id":"pAmaRU92mGyT","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"2fc04d3e-147f-4e15-b014-7ff657e7a29c","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:36.176222Z","iopub.execute_input":"2025-01-09T12:02:36.176499Z","iopub.status.idle":"2025-01-09T12:02:36.319105Z","shell.execute_reply.started":"2025-01-09T12:02:36.176451Z","shell.execute_reply":"2025-01-09T12:02:36.318221Z"}},"outputs":[{"name":"stdout","text":"Label encoding complete. Here are the first few encoded labels:\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"            Product  Label\n0  Credit Reporting      3\n1      Student Loan     10\n2  Credit Reporting      3\n3  Credit Reporting      3\n4  Credit Reporting      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Credit Reporting</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Student Loan</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Credit Reporting</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Credit Reporting</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Credit Reporting</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def preprocess_text(text):\n\n    text = BeautifulSoup(text, \"html.parser\").get_text()\n\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n\nmerged_df['Cleaned_Complaint'] = merged_df['Consumer complaint narrative'].fillna('').apply(preprocess_text)\n\n\nmerged_df['Word_Count'] = merged_df['Cleaned_Complaint'].apply(lambda x: len(x.split()))\nfiltered_df = merged_df[merged_df['Word_Count'] >= 10]\nprint(f\"Dataset shape after removing very short complaints: {filtered_df.shape}\")\n\n\nprint(\"\\nSample of preprocessed data:\")\nfiltered_df[['Cleaned_Complaint', 'Word_Count', 'Label']].head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"7MHOZ4OL5wAo","outputId":"6cc74835-40ed-4ec8-b46d-b2f96d17f5f5","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:02:36.320937Z","iopub.execute_input":"2025-01-09T12:02:36.321165Z","iopub.status.idle":"2025-01-09T12:04:33.956773Z","shell.execute_reply.started":"2025-01-09T12:02:36.321145Z","shell.execute_reply":"2025-01-09T12:04:33.955993Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-10-f89f8a92282b>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  text = BeautifulSoup(text, \"html.parser\").get_text()\n","output_type":"stream"},{"name":"stdout","text":"Dataset shape after removing very short complaints: (933073, 5)\n\nSample of preprocessed data:\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                   Cleaned_Complaint  Word_Count  Label\n0  My credit reports are inaccurate These inaccur...          39      3\n1  Beginning in XXXXXXXX I had taken out student ...         323     10\n2  I am disputing a chargeoff on my account that ...          69      3\n3  I did not consent to authorize nor benefit fro...          99      3\n4  I am a federally protected consumer and I am a...         430      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cleaned_Complaint</th>\n      <th>Word_Count</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My credit reports are inaccurate These inaccur...</td>\n      <td>39</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Beginning in XXXXXXXX I had taken out student ...</td>\n      <td>323</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am disputing a chargeoff on my account that ...</td>\n      <td>69</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I did not consent to authorize nor benefit fro...</td>\n      <td>99</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I am a federally protected consumer and I am a...</td>\n      <td>430</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## 1.5 Dataset Creation and Tokenization\n\nFor training our BERT model, we need to:\n1. Create a custom Dataset class that will handle tokenization\n2. Split the data into training and testing sets\n3. Use BERT's tokenizer to convert text into a format suitable for the model","metadata":{"id":"j4jVvN4oopUU"}},{"cell_type":"code","source":"class ComplaintDataset(Dataset):\n    \"\"\"\n    A custom Dataset class for handling consumer complaints text data with BERT tokenization.\n\n    Parameters:\n        texts (List[str]): List of complaint texts to be processed\n        labels (List[int]): List of encoded labels corresponding to each text\n        tokenizer (BertTokenizer): A BERT tokenizer instance for text processing\n        max_len (int, optional): Maximum length for padding/truncating texts. Defaults to 512.\n\n    Returns:\n        dict: For each item, returns a dictionary containing:\n            - input_ids (torch.Tensor): Encoded token ids of the text\n            - attention_mask (torch.Tensor): Attention mask for the padded sequence\n            - labels (torch.Tensor): Encoded label as a tensor\n    \"\"\"\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n","metadata":{"id":"yHLQgJhopEh5","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:04:33.957917Z","iopub.execute_input":"2025-01-09T12:04:33.958191Z","iopub.status.idle":"2025-01-09T12:04:33.963707Z","shell.execute_reply.started":"2025-01-09T12:04:33.958169Z","shell.execute_reply":"2025-01-09T12:04:33.962895Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    filtered_df['Cleaned_Complaint'].tolist(),\n    filtered_df['Label'].tolist(),\n    test_size=0.2,\n    random_state=42\n)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_dataset = ComplaintDataset(\n    texts=X_train,\n    labels=y_train,\n    tokenizer=tokenizer,\n    max_len=512\n)\ntest_dataset = ComplaintDataset(\n    texts=X_test,\n    labels=y_test,\n    tokenizer=tokenizer,\n    max_len=512\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False\n)\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of testing samples: {len(test_dataset)}\")\n","metadata":{"id":"-js5x8M5mksA","colab":{"base_uri":"https://localhost:8080/","height":310,"referenced_widgets":["73b707972feb43fbb12cf69cb5355c4d","2918e1e9113a4505aaef1a08e43aa831","a3c98f8ab89b470da132a0ecd330df06","233d606f46d240c782b3a8b788c773c2","5a93f52b57e842089e75b6dbb5b977a5","6fc5fb28327f41d598f465c3f7d0c056","c0b0712d9d834e74a05e7ffc64b4ee78","3242d33f28b446a1bcd8774cb49b8d3a","26b46e91437844b9a287b159f6165b6b","82eb9cada13b40b2859eb3f8f4c342ec","6f416dcbfcb141bf8177df2f34bc3158","facaf403897e4599b31a64967d284ae7","8b25a3fecf994184a38322ed4d06e970","dc1e88e0a4764c13b0789f5ac65476a1","ee1557e128fc40b8a4a1d349d027296c","3e3168e32ff545a5b9910c21f93acedb","932da40830274fb99bf2e4e0c603dec2","8a35fdf7e9d74791b806d8d37472a560","0ca64dc57a034ddba0ee30717669e442","845b8b99a9624a9e807d830cf2da178e","e6ffd3f63f054a8b8dfd6ab8aa685c52","7f4201f58cef491bae7bfe5e986a9bf7","f02fd62a23c84d64b29276ed416a4242","2744749ee8e5461eac45ede5ace2b0ba","fde93033c8fb49418560da16eda6f3f9","197ecd94028340fe9563b204f2794b97","1fa068ffd78d4d3eb33da754a3f629af","077edd9f32b94989a2de33cef9ada369","4e0e6b6c217843259e0f5794afa0a409","595d8bc667c741af8bb530fbd831d5d9","69cc16a8ae4744c38133169d70decf09","e07aa8912428436d91dca6fb0f7e1043","7c15e6d375b0419f9295a8643f2eb72c","2be5cd6b2a1b4b269904946662be8aab","7a279cce661242a6b163a509f54985c2","4b3c66662c5a42ffa06878ac94a3051f","02d782033a874e62a8f9b3e73d1c0473","a88cad9a904a40cf82ac3440f3cfc629","0fd8d4bbe7f2469297f2b226a8876e26","8788c3b5bee748b58536b89cca1cea03","0b70e1d4944846f5b09188e103c26174","1389a52ea82447d4a9e9947cb6872a0f","9c2dfd57a528415086d53a9fa73fab14","d1af3cb26a004c1e8784ee1357ed4783"]},"outputId":"adf0093f-3d8e-43c3-e3cb-ce467fb82852","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:04:33.964708Z","iopub.execute_input":"2025-01-09T12:04:33.965020Z","iopub.status.idle":"2025-01-09T12:04:35.860525Z","shell.execute_reply.started":"2025-01-09T12:04:33.964990Z","shell.execute_reply":"2025-01-09T12:04:35.859575Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0951bd8cf13c437f8d220571e07314d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"235b99b5a68741d888cad779be35ec64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30c58a3f593a443e9ca2d1651ab94d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad252f3937b94b56abae147610bbea03"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Number of training samples: 746458\nNumber of testing samples: 186615\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Part 2: Training a Small-Size BERT Model\n\nIn this part, we will explore how to build and train a small-sized BERT model for our classification task. Instead of using the full-sized BERT model, which is computationally expensive, we will create a smaller version using the Transformers library.","metadata":{"id":"aMcc2gsbt0iJ"}},{"cell_type":"code","source":"config = BertConfig.from_pretrained(\n    \"prajjwal1/bert-tiny\",\n    num_labels=len(filtered_df['Label'].unique()),\n    output_attentions=False,\n    output_hidden_states=False\n)\n\nmodel = BertForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", config=config)\n\nprint(model)\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nTotal number of trainable parameters: {total_params}\")","metadata":{"id":"3RS5oBz3qmvu","colab":{"base_uri":"https://localhost:8080/","height":988,"referenced_widgets":["e2938ae2372645bb8d802f5584a58264","772fd0c1f3224c3a8dcc7c98e165971a","c10c0b5b2ae24d709a96d01fa600b749","81a759535ea144c8ace92715cc7fd7c1","4a45fea2fa5441e0abc15137e5b1421f","ea2fb54ef8934b88bde0a94364613aa6","fbcf857dc50c45f2a41226287e67c18e","32a14f57bddc44efa7a909763f60f4ca","608f0f0f4133479ab29673fe939bafec","b35e44afa1954874b632cca1d1efba61","1e6a0501fc65467ba214a09daf70a81f","0e4f9dd7dc4f4b31956c8d884c55d48b","43e8eb0792d84975ae609ee5cffe2d84","69f34c518b6f49e6904d037b3ec41a4f","fd8906336229485fba037de182ff9265","bb2bea62ebdf4be69b8a61f78a0862f3","8824952366bf4301b561cc3a352790d0","f53816b45d1c4b3a8e8f645a11c7ca98","ff2cd36fe38b44429ee08e08c4ad9cde","1e18c04dd8f3405e94f44f65332ff841","ce12ed992d0942acb809f917e03c4c94","1c33426f0ec0411599ce1828ccb1823d"]},"outputId":"b45f5f7b-e3de-4355-c914-c9e017e42f00","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:04:35.862070Z","iopub.execute_input":"2025-01-09T12:04:35.862313Z","iopub.status.idle":"2025-01-09T12:04:36.751707Z","shell.execute_reply.started":"2025-01-09T12:04:35.862291Z","shell.execute_reply":"2025-01-09T12:04:36.751032Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ebac0290ec42cf94dcd96bb5d08ff3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92c5dacc5fa4ac9b1f98c00945874e6"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=128, out_features=12, bias=True)\n)\n\nTotal number of trainable parameters: 4387468\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"---\n\nNow that you have defined your model, it's time to train it!☠️\n\nTraining a model of this size can take some time, depending on the available resources. To manage this, you can train your model for just **2–3 epochs** to demonstrate progress. Here are some hints:\n- **Training Metrics:** Ensure you print enough metrics, such as loss and accuracy, to track the training progress.\n- **Interactive Monitoring:** Use the `tqdm` library to display the progress of your training loop in real-time.","metadata":{"id":"Xr4Z14a6wL2c"}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-vZynZhHM-O","outputId":"35db1f94-b701-428f-d884-57b4808a66dc","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:04:36.752729Z","iopub.execute_input":"2025-01-09T12:04:36.753044Z","iopub.status.idle":"2025-01-09T12:04:36.773001Z","shell.execute_reply.started":"2025-01-09T12:04:36.753011Z","shell.execute_reply":"2025-01-09T12:04:36.772379Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-1): 2 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=128, out_features=12, bias=True)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 2\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch in tqdm(train_loader):\n\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n        loss.backward()\n        optimizer.step()\n\n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct / total\n    print(f\"Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy:.4f}\")\n\nprint(\"\\nEvaluating the model on the test dataset...\")\nmodel.eval()\ntotal_loss = 0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\navg_loss = total_loss / len(test_loader)\naccuracy = correct / total\nprint(f\"\\nTest Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n","metadata":{"id":"FRZW-F9Dw6AI","colab":{"base_uri":"https://localhost:8080/","height":459},"outputId":"9b3898fe-ce3f-4119-c715-269478703528","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:15:35.214730Z","iopub.execute_input":"2025-01-09T09:15:35.215023Z","iopub.status.idle":"2025-01-09T11:12:05.941410Z","shell.execute_reply.started":"2025-01-09T09:15:35.214999Z","shell.execute_reply":"2025-01-09T11:12:05.940506Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23327/23327 [51:46<00:00,  7.51it/s] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3501, Training Accuracy: 0.8858\n\nEpoch 2/2\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 23327/23327 [51:48<00:00,  7.50it/s] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.3227, Training Accuracy: 0.8938\n\nEvaluating the model on the test dataset...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5832/5832 [12:55<00:00,  7.52it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Loss: 0.3244, Test Accuracy: 0.8913\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Part 3: Fine-Tuning TinyBERT with LoRA\n\nAs you have experienced, training even a small-sized BERT model can be computationally intensive and time-consuming. To address these challenges, we explore **Parameter-Efficient Fine-Tuning (PEFT)** methods, which allow us to utilize the power of large pretrained models without requiring extensive resources.\n\n---\n\n### **Parameter-Efficient Fine-Tuning (PEFT)**\n\nPEFT methods focus on fine-tuning only a small portion of the model’s parameters while keeping most of the pretrained weights frozen. This drastically reduces the computational and storage requirements while leveraging the rich knowledge embedded in pretrained models.\n\nOne popular PEFT method is LoRA (Low-Rank Adaptation).\n\n- **What is LoRA?**\n\nLoRA introduces a mechanism to fine-tune large language models by injecting small low-rank matrices into the model's architecture. Instead of updating all parameters during training, LoRA trains these small matrices while keeping the majority of the original parameters frozen.  This is achieved as follows:\n\n1. **Frozen Weights**: The pretrained weights of the model, represented as a weight matrix $ W \\in \\mathbb{R}^{d \\times k} $, remain **frozen** during fine-tuning.\n\n2. **Low-Rank Decomposition**:\n   Instead of directly updating $ W $, LoRA introduces two trainable matrices, $ A \\in \\mathbb{R}^{d \\times r} $ and $ B \\in \\mathbb{R}^{r \\times k} $, where $ r \\ll \\min(d, k) $.  \n   These matrices approximate the update to $ W $ as:\n   $$\n   \\Delta W = A \\cdot B\n   $$\n\n   Here, $ r $, the rank of the decomposition, is a key hyperparameter that determines the trade-off between computational cost and model capacity.\n\n3. **Adaptation**:\n   During training, instead of updating $ W $, the adapted weight is:\n   $$\n   W' = W + \\Delta W = W + A \\cdot B\n   $$\n   Only the low-rank matrices $ A $ and $ B $ are optimized, while $ W $ remains fixed.\n\n4. **Efficiency**:\n   Since $ r $ is much smaller than $ d $ and $ k $, the number of trainable parameters in $ A $ and $ B $ is significantly less than in $ W $. This makes the approach highly efficient both in terms of computation and memory.\n\n---\n\n###  **Fine-Tuning TinyBERT**\n\nFor this part, we will fine-tune **TinyBERT**, a distilled version of BERT, using the LoRA method.\n\n- **What is TinyBERT?**\n\nTinyBERT is a lightweight version of the original BERT model created through knowledge distillation. It significantly reduces the model size and inference latency while preserving much of the original BERT’s effectiveness. Here are some key characteristics of TinyBERT:\n- It is designed to be more resource-efficient for tasks such as classification, question answering, and more.\n- TinyBERT retains a compact structure with fewer layers and parameters, making it ideal for fine-tuning with limited computational resources.\n","metadata":{"id":"-yHtTYcpz6AW"}},{"cell_type":"markdown","source":"> Similar to the previous section, training this model might take some time. Given the resource limitations, you can train the model for just **2-3 epochs** to demonstrate the process.\n","metadata":{"id":"n_Og-pBeV5x6"}},{"cell_type":"code","source":"!pip uninstall llama-index-llms-huggingface -y\n!pip uninstall llama-index-llms-huggingface-api -y\n!pip uninstall huggingface-hub -y\n!pip uninstall peft -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:22:01.307951Z","iopub.execute_input":"2025-01-09T12:22:01.308279Z","iopub.status.idle":"2025-01-09T12:22:05.068658Z","shell.execute_reply.started":"2025-01-09T12:22:01.308249Z","shell.execute_reply":"2025-01-09T12:22:05.067803Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping llama-index-llms-huggingface as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping llama-index-llms-huggingface-api as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: huggingface-hub 0.27.1\nUninstalling huggingface-hub-0.27.1:\n  Successfully uninstalled huggingface-hub-0.27.1\nFound existing installation: peft 0.14.0\nUninstalling peft-0.14.0:\n  Successfully uninstalled peft-0.14.0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"!pip install llama-index-llms-huggingface-api==0.3.0\n!pip install llama-index-llms-huggingface==0.4.0\n!pip install huggingface-hub==0.23.5\n!pip install peft==0.11.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:22:26.162544Z","iopub.execute_input":"2025-01-09T12:22:26.162843Z","iopub.status.idle":"2025-01-09T12:22:51.127031Z","shell.execute_reply.started":"2025-01-09T12:22:26.162819Z","shell.execute_reply":"2025-01-09T12:22:51.125951Z"}},"outputs":[{"name":"stdout","text":"Collecting llama-index-llms-huggingface-api==0.3.0\n  Downloading llama_index_llms_huggingface_api-0.3.0-py3-none-any.whl.metadata (1.3 kB)\nCollecting huggingface-hub<0.24.0,>=0.23.0 (from llama-index-llms-huggingface-api==0.3.0)\n  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\nCollecting llama-index-core<0.13.0,>=0.12.0 (from llama-index-llms-huggingface-api==0.3.0)\n  Downloading llama_index_core-0.12.10.post1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (4.12.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (2.0.35)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (3.10.5)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.2.15)\nCollecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0)\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nCollecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpx (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (3.3)\nCollecting nltk>3.8.1 (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (10.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (2.9.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (9.0.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (0.8.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.16.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (4.0.3)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (2024.9.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface-api==0.3.0) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (3.1.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (3.23.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (3.7.1)\nCollecting httpcore==1.* (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0)\n  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface-api==0.3.0) (1.2.2)\nDownloading llama_index_llms_huggingface_api-0.3.0-py3-none-any.whl (5.0 kB)\nDownloading huggingface_hub-0.23.5-py3-none-any.whl (402 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: filetype, dirtyjson, nltk, h11, huggingface-hub, httpcore, httpx, llama-index-core, llama-index-llms-huggingface-api\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ntransformers 4.47.1 requires huggingface-hub<1.0,>=0.24.0, but you have huggingface-hub 0.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dirtyjson-1.0.8 filetype-1.2.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.23.5 llama-index-core-0.12.10.post1 llama-index-llms-huggingface-api-0.3.0 nltk-3.9.1\nCollecting llama-index-llms-huggingface==0.4.0\n  Downloading llama_index_llms_huggingface-0.4.0-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface==0.4.0) (0.23.5)\nRequirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface==0.4.0) (0.12.10.post1)\nCollecting text-generation<0.8.0,>=0.7.0 (from llama-index-llms-huggingface==0.4.0)\n  Downloading text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface==0.4.0) (2.4.1+cu121)\nRequirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0) (4.47.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (4.12.2)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (2.0.35)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (3.10.5)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (0.6.7)\nRequirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.2.15)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.0.8)\nRequirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.2.0)\nRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (0.28.1)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.6.0)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (3.3)\nRequirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.26.4)\nRequirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (10.4.0)\nRequirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (2.9.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (9.0.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (0.8.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (0.9.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.16.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface==0.4.0) (1.13.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface==0.4.0) (3.1.4)\nINFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\nCollecting transformers<5.0.0,>=4.37.0 (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0)\n  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0) (2024.9.11)\nCollecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0) (0.4.5)\nINFO: pip is looking at multiple versions of transformers[torch] to determine which version is compatible with other requirements. This could take a while.\nRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0) (0.34.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface==0.4.0) (5.9.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (4.0.3)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (8.1.7)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.4.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface==0.4.0) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (3.1.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.0.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (3.23.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface==0.4.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface==0.4.0) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-huggingface==0.4.0) (1.2.2)\nDownloading llama_index_llms_huggingface-0.4.0-py3-none-any.whl (11 kB)\nDownloading text_generation-0.7.0-py3-none-any.whl (12 kB)\nDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, text-generation, llama-index-llms-huggingface\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.1\n    Uninstalling transformers-4.47.1:\n      Successfully uninstalled transformers-4.47.1\nSuccessfully installed llama-index-llms-huggingface-0.4.0 text-generation-0.7.0 tokenizers-0.20.3 transformers-4.46.3\nRequirement already satisfied: huggingface-hub==0.23.5 in /usr/local/lib/python3.10/dist-packages (0.23.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (4.66.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.23.5) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.5) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.5) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.5) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.23.5) (2024.8.30)\nCollecting peft==0.11.0\n  Downloading peft-0.11.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (4.46.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (4.66.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (0.34.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.11.0) (0.23.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (2024.6.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.11.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.11.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.11.0) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.11.0) (2024.9.11)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.11.0) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.11.0) (1.3.0)\nDownloading peft-0.11.0-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom peft import get_peft_model, LoraConfig, TaskType\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"id":"fe1vGCZwU7MZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:23:12.258113Z","iopub.execute_input":"2025-01-09T12:23:12.258452Z","iopub.status.idle":"2025-01-09T12:23:12.262621Z","shell.execute_reply.started":"2025-01-09T12:23:12.258422Z","shell.execute_reply":"2025-01-09T12:23:12.261867Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model_name = \"prajjwal1/bert-tiny\"\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(filtered_df['Label'].unique())\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nlora_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=4,\n    lora_alpha=16,\n    lora_dropout=0.1,\n)","metadata":{"id":"LIyN5vOLLWz6","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:23:18.030173Z","iopub.execute_input":"2025-01-09T12:23:18.030445Z","iopub.status.idle":"2025-01-09T12:23:19.103013Z","shell.execute_reply.started":"2025-01-09T12:23:18.030424Z","shell.execute_reply":"2025-01-09T12:23:19.102360Z"},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"423b5aa0-330f-4d99-d01b-16dde87a7c9f"},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9490f41ea2a4008821f37f0ca55f5bf"}},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"lora_model = get_peft_model(base_model, lora_config)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlora_model.to(device)\n\ntotal_params = sum(p.numel() for p in lora_model.parameters())\ntrainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\nprint(f\"\\nTotal parameters: {total_params}\")\nprint(f\"Trainable parameters (LoRA): {trainable_params}\")\nprint(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n\noptimizer = AdamW(lora_model.parameters(), lr=2e-5)\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"id":"jMgwZ8YmLuZ_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"dd7e8221-af68-4920-e1fc-23d0e9731a77","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:23:28.796851Z","iopub.execute_input":"2025-01-09T12:23:28.797194Z","iopub.status.idle":"2025-01-09T12:23:29.251842Z","shell.execute_reply.started":"2025-01-09T12:23:28.797167Z","shell.execute_reply":"2025-01-09T12:23:29.250659Z"}},"outputs":[{"name":"stdout","text":"\nTotal parameters: 4391564\nTrainable parameters (LoRA): 4096\nPercentage of trainable parameters: 0.09%\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"num_epochs = 2  # Set the number of epochs to train\n\n# Training loop\nfor epoch in range(num_epochs):\n    lora_model.train()  # Set the model to training mode\n\n    total_loss = 0\n    correct_preds = 0\n    total_preds = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\"):\n        # Zero gradients before each batch\n        optimizer.zero_grad()\n\n        # Move batch to device (GPU/CPU)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = lora_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n\n        # Track loss\n        total_loss += loss.item()\n\n        # Compute accuracy\n        predictions = torch.argmax(logits, dim=-1)\n        correct_preds += (predictions == labels).sum().item()\n        total_preds += labels.size(0)\n\n    # Compute average loss and accuracy for this epoch\n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct_preds / total_preds\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    print(f\"Training loss: {avg_loss:.4f}\")\n    print(f\"Training accuracy: {accuracy * 100:.2f}%\")\n\n\n\n# TODO : Evaluate the model on test dataset\n# Evaluation loop\nlora_model.eval()  # Set the model to evaluation mode\n\ncorrect_preds = 0\ntotal_preds = 0\ntest_loss = 0\n\n# Disable gradient calculation for evaluation\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = lora_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        # Track test loss\n        test_loss += loss.item()\n\n        # Compute accuracy for the batch\n        predictions = torch.argmax(logits, dim=-1)\n        correct_preds += (predictions == labels).sum().item()\n        total_preds += labels.size(0)\n\n# Compute average test loss and accuracy\navg_test_loss = test_loss / len(test_loader)\ntest_accuracy = correct_preds / total_preds\n\nprint(f\"\\nEvaluation Results:\")\nprint(f\"Test loss: {avg_test_loss:.4f}\")\nprint(f\"Test accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"id":"J395FrcWMbmx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c57f15cd-695e-432b-f08c-6450e5d63055","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:23:30.240195Z","iopub.execute_input":"2025-01-09T12:23:30.240764Z","iopub.status.idle":"2025-01-09T14:43:00.488653Z","shell.execute_reply.started":"2025-01-09T12:23:30.240733Z","shell.execute_reply":"2025-01-09T14:43:00.487643Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/2: 100%|██████████| 23327/23327 [1:03:20<00:00,  6.14batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2\nTraining loss: 1.9463\nTraining accuracy: 61.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2: 100%|██████████| 23327/23327 [1:03:10<00:00,  6.15batch/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/2\nTraining loss: 1.8861\nTraining accuracy: 65.97%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 5832/5832 [12:59<00:00,  7.48batch/s]","output_type":"stream"},{"name":"stdout","text":"\nEvaluation Results:\nTest loss: 1.8298\nTest accuracy: 67.45%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"id":"PDmB6bqn5m-X"},"outputs":[],"execution_count":null}]}